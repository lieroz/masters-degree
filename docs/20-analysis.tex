\chapter{Аналитический раздел}
\label{cha:analysis}
\section{Цель и задачи работы}
Целью данной работы является создание метода управления памятью в многопоточных приложениях.
Для достижения данной цели необходимо решить следующие задачи:
\begin{itemize}
	\item проанализировать предметную область и существующие методы выделения оперативной памяти приложениям;
	\item разработать метод выделения оперативной памяти приложениям;
	\item создать ПО, тесттирующее различные методы выделения памяти;
	\item создать ПО, реализующее  разработанный метод выделения оперативной памяти.
\end{itemize}

\section{Сложность выделения памяти}

Наименьшая единица адресации памяти в современных операционных системах - страница. Ее размер обычно равен 4КБ, но есть, так называемые, большие   страницы, размер которых варьируется на каждой архитектуре и для x86 состовляет 2МБ. Этот размер ограничен базовым оборудованием. Это означает, что операционная   система не может запросить памяти меньше чем размер страницы. Процессор не умеет работать с памятью   линейно, для получения физического адрес, он делает запрос в УУП (англ. MMU)   (который   по   сутия вляется кешом TLB) и далее уже через него получает нужный адрес. Это тоже своего рода проблема, потому что линейный доступ к памяти обеспечил бы минимум необходимых операций по трансляции виртуальных адресов в физические, но тогда пришлось бы переделывать все что связано с виртуальной памятью   процесса, а значит и переделывать сами процессоры и операционные системы.

Процесс может запросить память у ОС через 2 системных вызова: \textbf{\textit{mmap}} и \textbf{\textit{sbrk}}.

\textbf{\textit{sbrk}} управляет точкой разрыва программы, тоесть функция   опеределяет доступную процессу память просто инкрементируя указатель на свободную память в куче до тех пока, пока не достигнет области отображаемой в процесс памяти.

\textbf{\textit{mmap}} отображает в адресное пространство процесса системные страницы памяти.

Почти все существующие библиотеки отказались от использования \textbf{\textit{sbrk}} в пользу \textbf{\textit{mmap}} по следующим причинам:
\begin{itemize}
	\item у sbrk есть ограничение в виде лимита выделения памяти, данный лимит - это ближайшая к нему отображаемая область памяти;
	\item память выделяется только линейно, нельзя выделить в любом месте кучи как это умеет делать mmap.
\end{itemize}

Вызов \textbf{\textit{mmap}} занимает больше времени, но он более гибок и универсален в использовании. Но одними системными вызовами не получится эффективно управлять оперативной памятью. Каждый системный вызов генерирует прерывание, которое приостанавливает код в пространстве пользователя и перемещается   на исполнение кода в пространстве  ядра, это называется сменой контекста. Смена контекста процесса является тяжелой операцией, которая требует сохранить текущее состояние процесса. Поэтому все библиотеки стараются делать эти вызовы как можно реже и запрашивать у ОС как можно больше памяти за один системный вызов.

\section{Glibc malloc}
Glibc является стандартной библиотекой языка С для систем, построенных на базе ядра Linux. В ней же реализован и стандартный системный аллокатор для програм пользователей, который доступен через интерфейсы malloc/free.

\subsection{Общее описание}
Изначально, в качестве стандартного системного аллокатора, испольщовалась библиотека dlmalloc, но она не поддерживала работу с потоками, поэтому была заменена на ptmalloc2, которая в дальнейшем стала частью glibc и теперь является стандартным аллокатором памяти. Поддержка работы с потоками помогает повысить производительность аллокатора и, следовательно, производительность приложений. В dlmalloc, когда два потока вызывают malloc одновременно, только один поток может войти в критическую секцию, так как свободный список памяти разделен между всеми потоками. Следовательно, выделение памяти занимает много времени в многопоточных приложениях, что приводит к снижению производительности. В то время как в ptmalloc2, когда два потока вызывают \textbf{\textit{malloc}} одновременно, память выделяется быстрее, так как каждый поток поддерживает отдельный сегмент кучи и для каждой кучи имеется свой список свободной памяти. Этот акт поддержания отдельных структур данных кучи и свободного списка для каждого потока называется ареной потока.

\subsection{Арена}
Как уже было сказано, арена это структура данных, которая отвечает за распределение памяти определенному количеству потоков. Есть два вида арен:
\begin{itemize}
	\item главная арена;
	\item арена под несколько потоков.
\end{itemize}
Главная арена создается основном потоком исполнения и управляет кучей через системный вызов \textbf{\textit{sbrk}}, тогда как потоковая арена запрашивает память у ОС через \textbf{\textit{mmap}}. При этом, если пользователь запросил память размером более чем 128КБ и в текущей куче ее недостаточно для удовлетворения запроса, память выделяется через системный вызов \textbf{\textit{mmap}}, вне зависимости от того в какую арену пришел запрос на выделение. Но выделять отдельную арену под каждый поток может требовать много ресурсов, поэтому они не создаются бесконечно, Поэтому количество активных арен для одного процесса ограничено значениями:
\begin{itemize}
	\item для 32-ух разрядных систем - 2 * количество логических ядер в системе;
	\item для 64-ех разрядных систем - 8 * количество логических ядер в системе.
\end{itemize}
Например, если есть многопоточное приложение (4 потока, основной + 3 пользовательских) запущенное но 32-ух битной системе, на которой есть 1 процессорное ядро. Количество потоков, в данном случае, больше лимита арен, которое доступно процессу на этом ПК (4 > 2 * 1). В таком случае библиотека должна гарантировать разделение арен между несколькими потоками:
\begin{itemize}
	\item основной поток исполнения вызывает \textbf{\textit{malloc}}, используется уже созданная главная арена без каких-либо блокировок;
	\item потоки 1 и 2 вызывают \textbf{\textit{malloc}}, для каждого создается отдельная арена без каких-либо блокировок, на данный момент количество арен равно количеству потоков;
	\item поток 3 вызывает \textbf{\textit{malloc}}, лимит арен на процесс в системе достигнут, поэтому \textbf{\textit{malloc}} попробует переиспользовать уже созданные арены:
	\begin{itemize}[$\star$]
		\item идет линейный проход по всем аренам, в этот момент производится попытка захватить мьютекс для сериализации доступа;
		\item если удалось захватить ресурс, то память возвращается польpователю (к примеру удалось захватить главную арену);
		\item если не удалось, то произойдет попытка захвата следующей арены.
	\end{itemize}
	\item теперь, когда 3-ий поток вызвал \textbf{\textit{malloc}} 2-ой раз, тот попытается переиспользовать арену, которая была испольщована в предыдущий раз (в данном случае это главная арена), еслт она недоступна, поток блокируется до тех пор, пока арена не освободится, получается что главная арена разделена между основном и 3-им потоками.
\end{itemize}

\subsection{Множественные кучи}
В библиотеке есть 3 основных структуры данных:
\begin{itemize}
	\item \textit{heap\_info}, заголовок кучи, одна арена потока может иметь несколько куч, каждая куча имеет свой собственный заголовок, потому что они состоят из разных отображаемых областей, и у каждой такой области будет свой заголовок;
	\item \textit{malloc\_state}, заголовок арены, у одной арены может быть множество куч, но для них всех определен только один заголовок арены;
	\item \textit{malloc\_chunk}, заголовок блока, куча разделена на множество блоков и у каждого есть свой заголовок.
\end{itemize}
У главной арены нет множества куч и поэтому нет и структуры \textit{heap\_info}, когда главная арена заканчивается, то инкреметрируется указатель разрыва программы до тех пор, пока не упрется в область отображаемой памяти. В отличии от потоковой арены, заголовок арены главной арены - это глобальная переменная, которую можно найти в сегменте данных файла libc.so.

\begin{figure}[!h]
	\begin{minipage}{0.5\textwidth}
		\includegraphics[scale=0.5]{images/glibc-malloc-main-arena.png}
		\caption{Диаграмма главной арены.}
		\label{glibc-malloc-main-arena}
	\end{minipage}
	\begin{minipage}{0.5\textwidth}
		\includegraphics[scale=0.4]{images/glibc-malloc-thread-arena.png}
		\caption{Диаграмма потоковой арены.}
		\label{glibc-malloc-thread-arena}
	\end{minipage}
\end{figure}

\begin{figure}[!h]
	\begin{center}
		\includegraphics[scale=0.6]{images/glibc-malloc-thread-arena-multi.png}
		\caption{Диаграмма потоковой арены с несколькими кучами.}
		\label{glibc-malloc-thread-arena-multi}
	\end{center}
\end{figure}

\subsection{Блоки}
Есть несколько типов блоков, которые можно найти в сегменте кучи:
\begin{itemize}
	\item выделенный блок;
	\item свободный блок;
	\item верхний блок;
	\item последний блок.
\end{itemize}

\subsubsection{Выделенный блок}

\textit{\textbf{prev\_size}} - если предыдущий фрагмент свободен, то это поле содержит размер предыдущего фрагмента. В противном случае, если выделен предыдущий фрагмент, это поле содержит пользовательские данные предыдущего фрагмента.

\textit{\textbf{size}} - это поле содержит размер выделенного фрагмента. Последние 3 бита этого поля содержат информацию о флаге.
\begin{itemize}
	\item \textbf{PREV\_INUSE(P)} - этот бит устанавливается при выделении предыдущего блока;
	\item \textbf{IS\_MAPPED(M)} - этот бит выставлен если блок является отображаемым в память процесса через \textbf{\textit{mmap}};
	\item \textbf{NON\_MAIN\_ARENA(N)} - этот бит выставлен когда блок принадлежит потоковой арене.
\end{itemize}
Другие поля \textit{malloc\_chunk} (например, fd, bk) не используются для выделенного фрагмента. Следовательно, вместо этих полей хранятся пользовательские данные. Запрошенный пользователем размер преобразуется в полезный размер (внутренний размер представления), так как требуется некоторое дополнительное пространство для хранения \textit{malloc\_chunk}, а также для целей выравнивания. Преобразование происходит таким образом, что последние 3 бита полезного размера никогда не задаются и, следовательно, используются для хранения информации о флаге.

\subsubsection{Свободный блок}

\textit{\textbf{prev\_size}} - никакие два свободных блока не могут соседствовать друг с другом, когда оба блока свободны, оин объединяется в один свободный блок большего размера. Следовательно, всегда будет выделен предыдущий фрагмент для этого освобожденного фрагмента, и поэтому \textit{prev\_size} содержит пользовательские данные предыдущего фрагмента.

\textit{\textbf{size}} - это поле содержит размер свободного блока.

\textit{\textbf{fd}} (forward pointer) - указывает на следующий фрагмент в той же ячейке (а не на следующий фрагмент, присутствующий в физической памяти).

\textit{\textbf{bk}} (backward pointer) - указывает на предыдущий фрагмент в той же ячейке (а не на предыдущий фрагмент, присутствующий в физической памяти).

\begin{figure}[!h]
	\begin{minipage}{0.5\textwidth}
		\includegraphics[scale=0.5]{images/glibc-malloc-allocated-chunk.png}
		\caption{Выделенный блок.}
		\label{glibc-malloc-allocated-chunk}
	\end{minipage}
	\begin{minipage}{0.5\textwidth}
		\includegraphics[scale=0.5]{images/glibc-malloc-free-chunk.png}
		\caption{Свободный блок.}
		\label{glibc-malloc-free-chunk}
	\end{minipage}
\end{figure}

\subsection{Бины}
Бины это структуры данных в свободном списке, которые хранят свободные блоки. В зависимости от размеров блоков, есть несколько типов бинов:
\begin{itemize}
	\item быстрые бины;
	\item неотсортированные бины;
	\item малые бины;
	\item большие бины.
\end{itemize}

В библиотеке есть 2 структуры данных, которые хрянят эти бины:
\begin{itemize}
	\item fastbinsY - этот массив содержит быстрые бины;
	\item bins - этот массив содержит неотсортированные, малые и большие бины, всего их 126 и они разделены следующим образом: 1-ый бин - неотсортированный, бины со 2-го по 63-ий - малые, бины с 64-го по 126-ой - большие.
\end{itemize}

\subsubsection{Быстрые бины}
Блоки размером от 16 до 80 байт называются быстрами. Ячейки, которые содержат такие блоки называются быстрыми бинами. Среди всех видом ячеек, быстре бины быстрее всего выделяют и освобождают память.

\begin{figure}[!h]
	\begin{center}
		\includegraphics[scale=0.6]{images/glibc-malloc-fast-bin-snap.png}
		\caption{Список быстрых бинов.}
		\label{glibc-malloc-fast-bin-snap}
	\end{center}
\end{figure}

\subsubsection{Неотсортированные бины}
Когда малый или большой блоки освобождаются, они добавляются в список неотсортированных бинов. Такой подход позволяет библиотеке переиспользовать недавно освобожденные блоки. Поэтому скорость выделения и освобождения памяти немного возрастает, потому что исчезает время на поиск подходящего бина.

\begin{figure}[!h]
	\begin{center}
		\includegraphics[scale=0.6]{images/glibc-malloc-all-bins.png}
		\caption{Список неотсортированный, мылых и больших бинов.}
		\label{glibc-malloc-all-bins}
	\end{center}
\end{figure}

\subsubsection{Малые бины}
Блоки размером менее чем 512 байт называются малыми. Бины, содержащие маоые блоки, называются малыми бинами. Малые бины быстрее чем большие, но медленнее чем быстрые в выделении и освобождении памяти.

\subsubsection{Большие бины}
Блоки размером более чем 512 байт называются большими. Бины, которые соджержат большие блоки, называются большими бинами. Эти бины выделяют память медленнее всего.

\subsubsection{Верхний блок}
Блок, находящийся на верхней границе арены, называется верхним блоком. Он не принадлежит ни одному бину и используется для обработки запроса пользователя когда нет свободных блоков ни в одном из бинов. Если размер верхнего блока больше чем запрашиваемый пользователем, он делится на 2 блока:
\begin{itemize}
	\item пользовательский блок нужного размера;
	\item оставшаяся часть блока.
\end{itemize}
Оставшийся блок становится новым верхним блоком, если его размер менее чем запрашиваемый польщователем, память увеличивается через вызов \textit{\textbf{sbrk}} (главная арена) или \textit{\textbf{mmap}} (потоковая арена).

\subsubsection{Последний оставшийся блок}
Это оставшийся блок после последнего разделения. Он позволяет улучшить локальность ссылки, тоесть последовательные запросы на выделение памяти могут оказаться выделенными рядом. Последним оставшимся блоком называется тот, который, в случае когда пользовательский запрос не может быть выполнен с использованием неотсортированного или малого бинов, тогда список больших бинов сканируется для того, чтобы найти следующий по величине не пустой бин. Когда такой блок найден, он разбивается на 2, польщовательский блок возвращается польщователю, а остаток помешается в список неотсортированных бинов и становится последним оставшемся блоком. Теперь, когда польщователб будет последовательно запрашивать малые блоки и последний оставшейся блок является единственным блоков в списке неотсортированных бинов, он разбивается на 2 блока, один вернется пользователю, другой станет новым оставшимся блоком и помещается в список неотсортированных бинов. Поэтому последовательные аллокации оказываются рядом друг с другом.

\section{TCMalloc}
TCMalloc это аллокатор памяти, разработанный компанией Google и являющийся заменой стандартному системному аллокатору, он имеет следующие характеристики:
\begin{itemize}
	\item Быстрое выделение и освобождение памяти для большинста объектов. Обекты кэшируются в зависимости от режима, либо кэш в каждом потоке, либо кэш на логический процессор. Большинство аллокаций не требуют синхронизации доступа к ресурсам, благодаря этому конкуренция за выделение памяти сводится к минимуму и обеспечивает хорошее масштабирование для многопоточных приложений.
	\item Гибкое использование памяти, освобожденная память может быть переиспользована для объектов различных размеров или возвращена системе.
	\item Низкие накладные расходы на память под каждый объект за счет выделения ``страниц'' объектов одинакового размера. Это позволяет хранить небольшие объекты в памяти эффективно и помогает избежать излишней фрагментации памяти.
	\item Низкая стоимость сэмлирования, которое позволяет получить детальное представление об использовании памяти приложением.
\end{itemize}

\subsection{Общее описание}

\begin{figure}[!h]
	\begin{center}
		\includegraphics[scale=0.6]{images/tcmalloc-overview.png}
		\caption{Внутренняя структура TCMalloc.}
		\label{tcmalloc-overview}
	\end{center}
\end{figure}

Библиотеку TCMalloc можно разбить на 3 основных компонента. Фронт-энд (англ. front-end), миддл-энд (англ. middle-end) и бэк-энд (англ. back-end):
\begin{itemize}
	\item Фронт-энд является кэшом, который предоставляет быстрое выделение и освобождение памяти приложению.
	\item Миддл-энд отвечает за наполнение фронт-энд кэшей.
	\item Бэк-энд управляет памятью, которая выделяется приложению самой ОС.
\end{itemize}

Фронт-энд может использоваться как в режиме кэш на каждый поток, так и в режиме кэш на каждый логический процессор. Бэк-энд может поддерживать работу с кучей, которая осведомлена о больших страницах.

\subsection{TCMalloc фронт-энд}

Фронт-энд обрабатывает запрос на выделение памяти определенного размера. У него имеется кэш памяти, который может использоваться для выделения или хранения свободной памяти. Этот кэш доступен только одному потоку одновременно, поэтому никаких блокировок не требуется и большинство выделений и освобождений памяти выполняются быстро.

Фронт-энд удовлетворит любой запрос, если у него есть закэшированная память соответствующего размера. Если кэш для этого конкретного размера пуст, будет сделан запрос на пополнение кэша в миддл-энд, который включает в себя центральный список свободной памяти и кэш передачи.

Если миддл-энд исчерпан или запрошенный размер больше максимального размера, который кэшируется фронт-эндом, запрос будет переадресован бек-энду, чтобы либо удовлетворить большое выделение памяти, либо пополнить кэш в миддл-энде. Бэк-энд также называется кучей страниц (англ. pageheap).

На данный момент существует 2 реализации фронт-энда:
\begin{itemize}
	\item Первоначально он поддерживал кэширование объектов по каждому потоку (отсюда и название англ. Thread Caching Malloc). Однако это могло приводить к появлению больних областей памяти, которые масштабировались с увеличением количества потоков. Современные приложения могут иметь большое количество потоков, что приводит либо к большим объемам памяти для каждого потока, либо к тому, что многие потоки имеют крошечные локальные кэши.
	\item Совсем недавно TCMalloc начал поддерживать режим кэширования под каждый логический процессор. В этом режиме каждый процессор в системе имеет свой собственный кэш, из которого выделяется память. Примечание: На x86 логический процессор эквивалентен гиперпотоку.
\end{itemize}

Различия между режимами работы кешей по потокам и процессорам полностью ограничиваются реализациями malloc/new и free/delete.

\subsubsection{Выделение памяти под большие и малые объекты}

Размер на выделение памяти под малые объекты сопоставляется c одним из 60-80 распределяемых размерных классов. Например, выделение 12 байт будет округлено до 16-ти байтового класса размера. Размерные классы предназначены для минимизации объема памяти, который тратится впустую при округлении до следующего по величине размерного класса.

При компиляции с  \_\_STD CPP\_DEFAULT\_NEW\_ALIGNMENT\_\_ <= 8 используется набор размеров, выровненных по 8-ми байтам. Это меньшее выравнивание сводит к минимуму потери памяти для многих распространенных размеров распределения (24, 40 и т.д.), которые в противном случае округляются до кратного 16-ти байтам. Во многих компиляторах это поведение контролируется флагом \textbf{\textit{-fnew-alignment=...}}. Когда \_\_STD CPP\_DEFAULT\_NEW\_ALIGNMENT\_\_ не указан (или больше 8 байт), использются стандартные 16-ти байтовые выравнивания для \textit{\textbf{::operator new}}. Однако при выделении менее 16-ти байт может быть возвращен объект с меньшим выравниванием, так как ни один объект с большим требованием выравнивания не может быть выделен в пространстве.

Когда запрашивается объект заданного размера, запрос сопоставляется с определенным размерным классом с помощью функции \textit{\textbf{SizeMap::GetSizeClass()}}, и возвращаемая память принадлежит этому классу. Это означает, что возвращаемая память по крайней мере так же велика, как и запрошенный размер. Выделения памяти из размерных классов обрабатываются фронт-эндом.

Объекты размером больше предела, определенного максимальным размером (\textbf{kMaxSize}), выделяются непосредственно из бэк-энда. Как таковые они не кэшируются ни фронт ни миддл эндами. Запросы на выделение больших размеров объектов округляются до размера страницы TCMalloc.

\subsubsection{Освобождение памяти}

Когда объект освобождается, компилятор предоставляет размер объекта, если он известен во время компиляции. Если размер не известен, он будет найден на карте страницы. Если объект мал, он будет помещен обратно в кэш фронт-энда. Если объект больше \textbf{\textit{kMaxSize}}, он возвращается непосредственно в кучу страниц.

\subsubsection{Режим кэша под каждый процессор}

В этом режиме выделяется один большой блок памяти. На следующей диаграмме показано как этот блок памяти разделен между процессорами и как каждый процессор использует часть блока для хранения метаданных, а также указателей на доступные объекты.

\begin{figure}[!h]
	\begin{center}
		\includegraphics[scale=0.6]{images/tcmalloc-per-cpu.png}
		\caption{Диграмма блока памяти при работе кэша в режиме под каждый процессор.}
		\label{tcmalloc-per-cpu}
	\end{center}
\end{figure}

Каждому логическому процессору назначается секция этой памяти для хранения метаданных и указателей на доступные объекты определенных размерных классов. Метаданные содержат один блок заголовок на каждый размерный класс. Заголовок содержит указатель на начало массива указателей на объекты размерных классов, а также указатель на текущую, динамическую, максимальную емкости и текущую позицию в этом сегменте массива. Статическая максимальная емкость каждого массива указателей для каждого размерного класса определяется во время запуска разницей между началом массива для этого класса и началом массива для следующего класса.

Во время выполнения максимальное количество элементов определенного размерного класса, которое может храниться в блоке для каждого процессора, будет варьироваться, но оно никогда не может превышать статически определенную максимальную емкость, назначенную при запуске.

Когда запрашивается объект определенного размерного класса, он удаляется из этого массива, а когда объект освобождается, он добавляется в массив. Если массив исчерпан, он снова заполняется запросом на выделение памяти в миддл-энд. Если массив переполняется, партия объектов удаляется из него и возвращается в миддл-энд.

Объем памяти, который может быть закэширован, ограничен для каждого процессора параметром \textbf{\textit{MallocExtension::SetMaxPerCpuCacheSize}}. Это означает, что общий объем кэшиуемой памяти зависит от количества активных кэшей под процессоры. Следовательно, машины с более высоким количеством процессоров могут кэшировать больше памяти.

Чтобы избежать удержания памяти на процессорах, где приложение больше не работает, вызывается функция \textbf{\textit{MallocExtension::ReleaseCpuMemory}}, которая освобождает объекты, хранящиеся в кэшах указанного процессора.

Внутри ЦП распределение памяти управляется по всем размерным классам таким образом, чтобы максимальный объем кэшируемой памяти оставался ниже предельно возможного. Тут стоит обратить внимание что он управляет максимальной суммой, которая может быть закэширована, а не суммой, которая в данный момент кэшируется. В среднем, фактически кэшируемая сумма должна составлять примерно половину лимита.

Максимальная емкость увеличивается в тот момент, когда в размерном классе заканчиваются объекты, а также при извлечении большого количества объектов рассматривается возможность увеличения емкости размерного класса. Емкость размерного класса может быть увеличена до тех пор, пока общая память (для всех размерных классов), которую может содержать кэш, не достигнет предела для каждого процессора или пока емкость этого размерного класса не достигнет жестко заданного предела размера для этого размерного класса. Если класс не достиг жестко заданного предела, то для увеличения емкости он может взять емкость из другого размерного класса на том же процессоре.

\subsubsection{Перезапускаемые последовательности и кэш под каждый процессор}

Для правильной работы, режим кэш под каждый процессор опирается на перезапускаемые последовательности (man rseq(2)). Перезапускаемая последовательность - это просто блок инструкций (на языке ассемблера), во многом похожий на типичную функцию. Ограничение перезапускаемых последовательностей состоит в том, что они не могут записывать частичное состояние в память, конечная инструкция должна быть единственной записью обновленного состояния. Идея перезапускаемых последовательностей заключается в том, что если поток удаляется из процессора (например, переключается контекст) во время выполнения перезапускаемой последовательности, то она будет перезапущена сначала. Следовательно, последовательность либо завершится сразу, либо будет многократно перезапущена, пока не завершится без прерываний. Это достигается без использования каких-либо блокирующих или атомарных инструкций, что позволяет избежать конкуренций за ресурсы в самой последовательности.

Практический смысл этого для TCMalloc заключается в том, что код может использовать перезапускаемую последовательность, такую как \textbf{\textit{TcmallocSlab\_Push}}, для извлечения или возврата элемента в массив для каждого процессора без необходимости блокировки. Перезапускаемая последовательность гарантирует, что либо массив обновляется без прерывания потока, либо последовательность перезапускается, если поток был прерван (например, переключателем контекста, который позволяет другому потоку работать на этом процессоре).

\subsubsection{Устаревший режим кэша под каждый поток}

В режиме кэша под каждый поток TCMalloc назначает каждому потоку локальный кэш. Небольшие выделения удовлетворяются из этого локального кэша. Объекты могут перемещаться между миддл-эндом в кэш для потока и из него по мере необходимости.

Потоковый кэш содержит один односвязный список свободных объектов на каждый размерный класс (так что если существует N размерных классов, то будет N соответствующих связных списков), как показано на следующей диаграмме.

\begin{figure}[!h]
	\begin{center}
		\includegraphics[scale=0.6]{images/tcmalloc-per-thread.png}
		\caption{Диграмма списка свободных объектов при работе кэша в режиме под каждый поток.}
		\label{tcmalloc-per-thread}
	\end{center}
\end{figure}

При выделении объект удаляется из соответствующего размерного класса кэшей для каждого потока. При освобождении объект добавляется к соответствующему списку размерного класса. Переполнение и опустошение обрабатываются путем запроса памяти у миддл-энда или возвращением туда избыточной памяти.

Максимальная емкость кэша для каждого потока устанавливается параметром \textbf{\textit{MallocExtension::SetMaxTotalThreadCacheBytes}}. Однако общий размер может превысить этот предел, так как каждый кэш для каждого потока имеет минимальный размер \textbf{\textit{KMinThreadCacheSize}}, который обычно составляет 512 КБ. В том случае, если поток хочет увеличить емкость своего кэша, ему необходимо уменьшить емкость кэша у других потоков.

По завершению работы потока, вся доступная ему память переходит в миддл-энд в кэш передачи.

\subsection{TCMalloc миддл-энд}

Миддл-энд отвечает за предоставление памяти фронт-энду и возврат памяти в бэк-энд. Он состоит из кэша передачи и центрального свободного списка. Хотя их часто называют сингулярными, существует один кэш передачи и один центральный свободный список на каждый размерный класс. Каждый из этих кэшей защищен мьютексом, поэтому доступ к ним требует затрат на сериализацию.

\subsubsection{Кэш передачи}

Когда фронт-энд запрашивает или возвращает память, он обращается к кэшу передачи. В нем содержится массив указателей на свободную память, и перемещение объектов в этот массив или извлечение объектов из этого массива по запросу фронт-энда происходят быстро.

Кэш передачи получил свое название из ситуаций, когда один поток выделяет память, освобожденную другим потоком. Он позволяет памяти быстро перемещаться между двумя различными потоками. Если кэш передачи не может удовлетворить запрос памяти или имеет недостаточно места для хранения возвращенных объектов, будет совершено обращение к центральному свободному списку.

\subsubsection{Центральный свободный список}

Центральный свободный список управляет памятью в спанах (англ. span), спан или диапазон - это набор из одной или нескольких страниц памяти TCMalloc. Запрос на один или несколько объектов удовлетворяется центральным свободным списком путем извлечения объектов из спанов до тех пор, пока запрос не будет удовлетворен. Если в диапазоне недостаточно доступных объектов, то из бэк-энда запрашиваются дополнительные спаны.

Когда объекты возвращаются в центральный свободный список, каждый объект сопоставляется с областью, к которой он принадлежит (с помощью карты страниц), а затем освобождается в эту область. Если все объекты, находящиеся в определенном диапазоне, возвращаются в него, то весь спан возвращается бэк-энду.

\subsubsection{Карты страниц и спаны}

Куча, управляемая TCMalloc, делится на страницы определенного размера во время компиляции. Область смежных страниц представлена объектом спан. Он может использоваться для управления большим объектом, который был передан приложению, или областью страниц, которые были разделены на последовательность небольших объектов. Если спан управляет малыми объектами, то размерный класс объектов записывается в него.

Карта страниц используется для поиска диапазона, к которому принадлежит объект, или для определения размерного класса для данного объекта. TCMalloc использует 2-ух уровневое или 3-ех уровневое компактное префиксное дерево (англ. radix tree) для отображения всех возможных ячеек памяти на спаны.

На следующей диаграмме показано, как 2-ух уровневая карта страниц используется для отображения адресов объектов на спаны, управляющие системными страницами, в которых находятся объекты. На диаграмме диапазон A занимает две страницы, а диапазон Б 3 страницы.

\begin{figure}[!h]
	\begin{center}
		\includegraphics[scale=0.6]{images/tcmalloc-pagemap-and-spans.png}
		\caption{Карта страниц сопоставляет объекты со спанами.}
		\label{tcmalloc-pagemap-and-spans}
	\end{center}
\end{figure}

Спаны используются в миддл-энде для определения места размещения возвращаемых объектов, и в бэк-энде для управления обработкой интервалов страниц.

\subsubsection{Хранение малых объектов в спанах}

Спан содержит указатели на адреса страниц, которыми он управляет. Для небольших объектов эти страницы разделены не более чем на $2^{16}$ объектов. Это значение выбрано таким образом, чтобы в пределах диапазона можно было ссылаться на объекты по двухбайтовому индексу. Это означает, развернутый связный список может быть использован для хранения объектов. Например, если есть 8-ми байтовые объекты, то можно хранить индексы трех готовых к использованию объектов и использовать четвертый слот для хранения индекса следующего объекта в цепочке. Эта структура данных уменьшает пропуски кэша по сравнению с полносвязным списком. Другое преимущество использования двухбайтовых индексов заключается в том, что можно использовать резервную емкость в самом спане для кэширования четырех объектов.

Когда нет доступных объектов определенного размерного класса, необходимо извлечь новый спан из кучи страниц и заполнить его.

\subsubsection{Размеры страниц в TCMalloc}

TCMalloc можно собрать с различными размерами страниц. Тут стоит отметить что они не соответствуют размерам страниц, которое используются в буффере ассоциативной трансляции (англ. TLB) базового оборудования. Эти размеры страниц в настоящее время составляют 4КБ, 8КБ, 32КБ и 256КБ.

Страница TCMalloc либо содержит несколько объектов определенного размера, либо используется как часть группы для хранения объекта размером больше одной страницы. Если вся страница становится свободной, она будет возвращена бек-энду в кучу страниц и позже может быть перепрофилирована для хранения объектов другого размера (или возвращена в ОС).

Небольшие страницы лучше справляются с требованиями приложения к памяти с меньшими накладными расходами. Например, наполовину использованная страница размером в 4КБ будет иметь 2КБ в остатке по сравнению с 32-ух килобайтной страницей, которая будет иметь 16КБ. Маленькие страницы также с большей вероятностью станут свободными. Например, страница в 4КБ может содержать восемь объектов по 512 байт по сравнению с 64-мя объектами на странице размером в 32КБ; и вероятность того, что 32 объекта будут свободны одновременно, намного меньше, чем вероятность того, что восемь станут свободными.

Большие страницы приводят к уменьшению потребности в извлечении и возврате памяти из бэк-энда. Одна страница размером 32КБ может содержать в восемь раз больше объектов, чем страница размером 4КБ, и это может привести к тому, что затраты на управление большими страницами будут меньше. Кроме того, требуется меньше больших страниц для отображения всего виртуального адресного пространства. TCMalloc имеет карту страниц, которая отображает виртуальный адрес на структуры, управляющие объектами в этом диапазоне адресов. Большие страницы означают, что карта страниц нуждается в меньшем количестве записей и, следовательно, она сама меньше.

Исходя из этого, для приложений с небольшими выделениями памяти или чувствительных к размеру областей памяти, имеет смысл использовать меньшие размеры страниц TCMalloc. Приложения с большим объемом памяти, скорее всего, выиграют от увеличения размеров страниц TCMalloc.

\subsection{TCMalloc бэк-энд }

У бэк-энда в TCMalloc есть 3 задачи:
\begin{itemize}
	\item Управление большими диапазонами неиспользуемой памяти.
	\item Извлечение памяти из операционной системы, когда нет подходящего размера памяти, доступной для выполнения запроса на выделение.
	\item Возврат ненужной памяти обратно в ОС.
\end{itemize}

Существует 2 реализации бэк-энда в TCMalloc:
\begin{itemize}
	\item Устаревшая куча страниц, которая управляет памятью в блоках размером со страницу TCMalloc.
	\item Куча страниц, которой известно о больших страницах, она управляет памятью блоками размером с большую страницу (в данном случае речь идет о размере страницы самого базового оборудования). Управление памятью большими страницами позволяет аллокатору повысить производительность приложения за счет уменьшения пропусков в буффере ассоциативной трансляции.
\end{itemize}

\subsubsection{Устаревшая куча страниц}

Устаревшая куча страниц - это массив свободных списков для определенных длин смежных страниц доступной памяти. Для всех индексов меньше 256-ти, список по индексу состоит из диапазонов, размер которых равен индексу умноженному на размер страницы TCMalloc. 256-я запись - это список диапазонов, длина которых >= 256 страниц:

\begin{figure}[!h]
	\begin{center}
		\includegraphics[scale=0.6]{images/tcmalloc-legacy-pageheap.png}
		\caption{Диаграмма устаревшей кучи страниц.}
		\label{tcmalloc-legacy-pageheap}
	\end{center}
\end{figure}

Выделение памяти для N страниц удовлетворяется путем поиска в N-м свободном списке. Если этот свободный список пуст, идет обращение в следующий свободный список и так далее. Если найти нужный диапазон памяти не удается, то память запрашивается у ОС через системный вызов \textbf{\textit{mmap}}.

Если выделение для N страниц удовлетворяется диапазоном страниц длиной большей N, то оставшаяся часть диапазона повторно вставляется обратно в соответствующий свободный список в куче страниц. Когда диапазон страниц возвращается в кучу, соседние страницы проверяются, чтобы определить, образуют ли они теперь непрерывную область, если это так, то страницы объединяются и помещаются в соответствующий свободный список.

\subsubsection{Аллокатор осведомленный о больших страницах}

Главная цель такого аллокатора - держать память в блоках состоящих из больших системных страниц. На архитектуре x86 размер такой страницы равен 2МБ. Для этого в бэк-энде есть 3 разных кэша:
\begin{itemize}
	\item Кэш наполнителя, содержит большие страницы, из которых была выделена некоторая память. Он имеет сходство с устаревшей кучей страниц в том смысле, что содержит связные списки памяти определенного количества страниц TCMalloc. Запросы на выделение ресурсов размером менее чем больщая страница (как правило) возвращаются из кэша наполнителя. Если кэш наполнителя не имеет достаточного количества доступной памяти, он запросит дополнительные большие страницы.
	\item Кэш регионов, который обрабатывает выделения размером более чем размер большой страницы. Данный кэш позволяет выделениям памяти распределяться между несколькими большими страницами и упаковывать несколько таких выделений в непрерывную область. Это особенно полезно для выделений, которые немного превышают размер огромной страницы (например, 2,1МБ).
	\item Кэш больших страниц обрабатывает выделения памяти размером, по крайней мере, с большую страницу. Существует перекрытие в использовании с кэшом регионов, но кэш регионов включается только тогда, когда точно определено (во время выполнения) что выделение памяти выиграет от этого.
\end{itemize}

\subsection{Рекомендации к использованию}

TCMalloc зарезервирует некоторую память для метаданных при запуске. Количество метаданных будет расти по мере роста кучи. В частности, карта страниц будет расти вместе с диапазоном виртуальных адресов, используемым TCMalloc, а спаны будут расти по мере роста числа активных страниц памяти. В режиме кэшей под каждый логический процессор TCMalloc резервирует блок памяти на процессор (обычно 256КБ), что в системах с большим количеством логических процессоров может привести к многомегабайтным областям в памяти.

Стоит отметить, что TCMalloc запрашивает память у ОС большими порциями (обычно 1ГБ). Адресное пространство зарезервировано, но не резервирует физическую память до тех пор, пока оно не будет использовано. Благодаря такому подходу, виртуальная память приложения (VSS) может быть существенно больше чем резидентная память (RSS). Побочным эффектом этого является то, что попытка ограничить использование памяти приложения путем ограничения VSS, потерпит неудачу задолго до того, как приложение использует столько физической памяти.

Библиотека TCMalloc не создана для того, чтобы использовать ее в уже работающем двоичный файле (например, используя JNI в Java-программах). Двоичный файл выделит некоторые объекты с помощью системного \textbf{malloc} и может попытаться передать их TCMalloc для освобождения. TCMalloc не сможет обрабатывать такие объекты.

\section{Jemalloc}
Проект jemalloc начинался как алокатор памяти для среды выполнения языка программирования, которую Джейсон Эванс разрабатывал в 2005 году, но изменения в дизайне языка сделали его излишним. В то время FreeBSD нуждалась в SMP-масштабируемом аллокаторе, поэтому он интегрировал jemalloc в libc FreeBSD, а затем сделал большое количество улучшений в масштабируемости и поведений фрагментации памяти.

В конце 2007 года проект Mozilla усердно работал над улучшением использования памяти Firefox для версии 3.0, и jemalloc был использован для решения проблем фрагментации Firefox на платформах Microsoft Windows. Многие общие улучшения jemalloc были результатом усилий, связанных с Firefox.

Начиная с 2009 года Джейсон Эванс адаптировал jemalloc для работы с экстремальными нагрузками, в которых обычно работают серверы Facebook, и добавил множество функций, поддерживающих разработку и мониторинг. Facebook использует jemalloc во многих компонентах, которые являются неотъемлемой частью обслуживания его веб-сайта, и по состоянию на 2017 год небольшая команда Facebook ведет постоянную разработку и техническое обслуживание.

Jemalloc - это универсальная реализация вызова \textit{\textbf{malloc}}, которая подчеркивает предотвращение фрагментации и масштабируемую поддержку параллелизма. Библиотека предназначен для использования в качестве системного аллокатора памяти во FreeBSD, но может быть иниегрирована практически в любую среду исполнения. Jemalloc предоставляет множество функций самоанализа, управления памятью и настройки, выходящих за рамки стандартных функций аллокаторов. В качестве крайнего примера арены могут быть использованы в качестве пулов памяти; то есть арена может быть использована для распределения общего назначения, а затем вся арена уничтожается одной операцией освобождения памяти.

\subsection{Детали реализации}
Данная библиотека использует несколько арен, чтобы уменьшить конкуренцию блокировок для многопоточных программ в многопроцессорных системах. Это хорошо работает с точки зрения масштабируемости потоков, но влечет за собой некоторые затраты. Существует небольшая фиксированная нагрузка на арену, и, кроме того, арены управляют памятью полностью независимо друг от друга, что означает небольшое фиксированное увеличение общей фрагментации памяти. Эти накладные расходы, как правило, не являются проблемой, учитывая количество обычно используемых арен. Использование значительно большего количества арен, чем по умолчанию, вряд ли улучшит производительность, главным образом из-за снижения производительности кэша. Однако, возможно, имеет смысл уменьшить количество арен, если приложение не очень часто использует функции аллокатора.

В дополнение к нескольким аренам этот аллокатор поддерживает потоковое кэширование, чтобы полностью избежать синхронизации для большинства запросов распределения. Такое кэширование позволяет очень быстро выделять ресурсы в обычном случае, но оно увеличивает использование памяти и фрагментацию, так как ограниченное количество объектов может оставаться выделенным в каждом потоковом кэше. В параметрах конфигурации можно указать использование арен под каждый логический процессор, в таком случае, количество арен будет раавно значение 2 * количество логических процессоров.

Память концептуально разбита на интервалы. Они всегда выровнены по кратным размеру страницы. Такое выравнивание позволяет быстро находить метаданные для пользовательских объектов. Пользовательские объекты разбиты на две категории по размеру: малые и большие. Примыкающие друг к другу малые объекты образуют блок, который находится в пределах одного интервала, в то время как большие объекты имеют свои собственные интервалы, поддерживающие их.

Небольшие объекты управляются группами по слабам. Каждый слаб поддерживает бптовый вектор, чтобы отслеживать используемые области. Запросы на выделение, которые составляют не более половины кванта (8 или 16, в зависимости от архитектуры), округляются до ближайшей степени двойки, которая составляет по крайней мере \textit{\textbf{sizeof(double)}}. Все остальные классы размеров объектов кратны кванту, разнесенные таким образом, что для каждого удвоения размера существует четыре класса размеров, что ограничивает внутреннюю фрагментацию примерно до \textbf{20\%} для всех, кроме самых малых размерных классов. Классы малого размера меньше четырехкратного размера страницы, а классы большого размера простираются от четырехкратного размера страницы до самого большого размерного класса, который не превышает значение \textit{\textbf{PTRDIFF\_MAX.}}

Выделения памяти плотно упакованы друг с другом, что может быть проблемой для многопоточных приложений. Если нужно убедиться, что аллокации не страдают от совместного использования кэш линий, надо округлить запросы на распределение до ближайшего кратного размера кэш линии или указать выравнивание кжш линии при выделении.

Если предположить, что размер страницы равен 4КБ и выравнивание выставлено в значение 16 байт на 64-ех битной системе, то размерные классы по каждой категории будут выглядеть как показано в таблице \ref{jemalloc-size-classes}.

\begin{table}[h!]
	\begin{center}
		\begin{tabular}{l|l|l}
			\textbf{Категория} & \textbf{Расстояние} & \textbf{Размеры}\\
			\hline
			\multirow{2}{*}{Малые} & 16 & [16, 32, 48, 64, 80, 96, 112, 128]\\
			& 32 & [160, 192, 224, 256]\\
			& 64 & [320, 384, 448, 512]\\
			& 128 & [640, 768, 896, 1024]\\
			& 256 & [1280, 1536, 1792, 2048]\\
			& 512 & [2560, 3072, 3584, 4096]\\
			& 1 КБ & [5 КБ, 6 КБ, 7 КБ, 8 КБ]\\
			& 2 КБ & [10 КБ, 12 КБ, 14 КБ]\\
			\hline
			\multirow{2}{*}{Большие} & 2КБ & [16КБ]\\
			& 4 КБ &	[20 КБ, 24 КБ, 28 КБ, 32 КБ]\\
			& 8 КБ & [40 КБ, 48 КБ, 54 КБ, 64 КБ]\\
			& 16 КБ & [80 КБ, 96 КБ, 112 КБ, 128 КБ]\\
			& 32 КБ & [160 КБ, 192 КБ, 224 КБ, 256 КБ]\\
			& 64 КБ & [320 КБ, 384 КБ, 448 КБ, 512 КБ]\\
			& 128 КБ & [640 КБ, 768 КБ, 896 КБ, 1 МБ]\\
			& 256 КБ & [1280 КБ, 1536 КБ, 1792 КБ, 2 МБ]\\
			& 512 КБ & [2560 КБ, 3 МБ, 3584 КБ, 4 МБ]\\
			& 1 МБ &	[5 МБ, 6 МБ, 7 МБ, 8 МБ]\\
			& 2 МБ &	[10 МБ, 12 МБ, 14 МБ, 16 МБ]\\
			& 4 МБ &	[20 МБ, 24 МБ, 28 МБ, 32 МБ]\\
			& 8 МБ &	[40 МБ, 48 МБ, 56 МБ, 64 МБ]\\
			&  ... & ...\\
			& 512 ПБ &	[2560 ПБ, 3 ЭБ, 3584 ПБ, 4 ЭБ]\\
			& 1 ЭБ	& [5 ЭБ, 6 ЭБ, 7 ЭБ]\\
		\end{tabular}
		\caption{Размерные классы.}
		\label{jemalloc-size-classes}
	\end{center}
\end{table}

\subsection{Основные алгоритмы и структуры данных}
Jemalloc объединяет несколько своих оригинальных идей с довольно большим набором идей, которые были впервые проверены в других аллокаторах памяти. Ниже приводится смесь этих идей и философии распределения, которые объединились, чтобы сформировать jemalloc.
\begin{itemize}
	\item малые объекты распределяются по размерным классам и используют нижние адреса при переиспольщовании, такая политика компоновки возникла в \textit{phkmalloc} и является ключом к предсказуемому поведению низкой фрагментации в \textit{jemalloc}.
	\item тщательно подобранные классы размеров, если расстояние между ними большое, то у объектов будет излишняя внутренняя фрагментация, при этом, при увеличении размера, внешняя фрагментация тоже увеличивается, потому что некторые объекты используется не полностью.
	\item накладываются жесткие ограничения на метаданные самого аллокатора, размер которых не превышает 2-ух \% от всей используемой памяти для всех размерных классов.
	\item минимизация активного набора страниц, ядра операционных систем управляют виртуальной паямтью через страницы, поэтому важно сконцентрировать все данные на как можно меньшем количестве страниц. \textit{phkmalloc} подтвердил этот принцип в то время, когда приложения обычно боролись с активными страницами, заменяемыми на жесткий диск, хотя это по-прежнему важно в современном контексте полного отказа от сваппинга.
	\item гонка за ресурсы сведена к минимуму, независимые арены \textit{jemalloc} были вдохновлены \textit{lkmalloc}, но со временем tcmalloc совершенно ясно дал понять, что еще лучше вообще избегать синхронизации, поэтому \textit{jemalloc} также реализует потоковое кэширование.
\end{itemize}

Виртуальная память в библиотеке логически разделена на блоки по 4МБ. В результате можно найти метаданные аллокатора для малых и больших объектов (внутренние указатели) за константное время при помощи манипуляций с указателями и найти метаданные для огромных объектов (выровненных по фрагментам) за логарифмическое время с помощью глобального красно-черного дерева.

Потоки приложений назначаются аренам в циклическом порядке при первом выделении малого/большого объекта. Арены полностью независимы друг от друга. Они поддерживают свои собственные блоки, из которых вырезают страницы для небольших/больших объектов. Освобожденная память всегда возвращается на арену, откуда она пришла, независимо от того, какой поток выполняет освобождение. При этом, в новых версиях библиотеки появилась поддержка арен под каждый логический процессор, обычно, если ОС поддерживает получение информации о номере ядра, на котором сейчас исполняется программа, то количество арен равно значению 2 * количество логических ядер. Таким образом, когда поток хочет выделить память, он найдет нжную арену по остатку от деления и выделения памяти будут равномерно распределены по всем кэшам.

\begin{figure}[!h]
	\begin{center}
		\includegraphics[scale=0.6]{images/jemalloc-arena-chunk.png}
		\caption{Вид блока арены в библиотеке jemalloc.}
		\label{jemalloc-arena-chunk}
	\end{center}
\end{figure}

Каждый фрагмент арены содержит метаданные (в основном карту страниц), за которыми следует один или несколько диапазонов страниц. Небольшие объекты группируются вместе с дополнительными метаданными в начале каждого диапазона страниц, в то время как большие объекты независимы друг от друга, и их метаданные полностью находятся в заголовке блока арены. Каждая арена отслеживает неполную страницу малого объекта через красно-черные деревья (по одному для каждого класса размера), и всегда обслуживает запросы на выделение, используя неполную страницу с наименьшим адресом для этого размерного класса. Каждая арена отслеживает доступную страницу через два красно-черных дерева — одно для чистых/нетронутых страниц и одно для грязных/тронутых страниц. Диапазоны страниц предпочтительно выделяются из грязного дерева с использованием наименьшего наилучшего соответствия.

\begin{figure}[!h]
	\begin{center}
		\includegraphics[scale=0.6]{images/jemalloc-arena-and-thread-cache-layout.jpg}
		\caption{Вид арены и потокового кэша в библиотеке jemalloc.}
		\label{jemalloc-arena-and-thread-cache-layout}
	\end{center}
\end{figure}

Каждый поток поддерживает кэш небольших объектов, а также больших объектов до ограниченного размера (по умолчанию 32 КБ). Таким образом, подавляющее большинство запросов на выделение сначала проверяют наличие кэшируемого объекта, прежде чем обращаться к арене. Распределение через кэш потоков не требует никакой блокировки, в то время как распределение через арену требует блокировки ячейки арены (по одной на класс малого размера) и/или арены в целом.

Основная цель потоковых кэшей (или кэшей на каждый процессор) - уменьшить объем событий синхронизации. Таким образом, максимальное количество кэшируемых объектов для каждого размерного класса ограничено на уровне, который позволяет на практике сократить синхронизацию в 10-100 раз. Более высокие ограничения кэширования еще больше ускорят выделение ресурсов для некоторых приложений, но в общем случае это приведет к неприемлемым затратам на фрагментацию. Чтобы еще больше ограничить фрагментацию, кэши выполняют инкрементную "сборку мусора" (GC), где время измеряется в терминах запросов на выделение. Закэшированные объекты, которые остаются неиспользуемыми для одного или нескольких проходов GC, постепенно сбрасываются на соответствующие арены с использованием экспоненциального подхода распада.